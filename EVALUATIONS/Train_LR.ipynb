{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "colab_type": "code",
    "id": "z6wkT68kSFhm",
    "outputId": "75a42950-2a74-4ad9-f967-408fab5cb52c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/pranavmittal/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression Model\n",
    "# Import libraries\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download(\"popular\")\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5Lg6YOOTebQ"
   },
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv('./final60_amzn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "colab_type": "code",
    "id": "7fC_MiiAYlXZ",
    "outputId": "425a397f-c8f0-4424-af30-f2e83e536e5b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>stock_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>nape summit week will also feature the annual ...</td>\n",
       "      <td>0.050660</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2017-12-07 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>from apples hugely anticipated iphone x to sam...</td>\n",
       "      <td>0.107128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-12-08 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>they are not just the largest browser, but the...</td>\n",
       "      <td>0.034394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017-12-12 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>the humanitarian crisis in the drc has placed ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2017-12-12 22:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>cramer prefers finisar (nasdaq: fnsr ) after a...</td>\n",
       "      <td>0.035844</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2017-12-14 18:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>4247</td>\n",
       "      <td>this report just confirmed apple's china probl...</td>\n",
       "      <td>0.013567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2019-02-01 22:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>4248</td>\n",
       "      <td>apple's (nasdaq: aapl ) spectacular rise and f...</td>\n",
       "      <td>0.017423</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2019-02-01 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>4249</td>\n",
       "      <td>so, lets see what to expect from the entertain...</td>\n",
       "      <td>0.029321</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2019-02-01 23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>4250</td>\n",
       "      <td>investorplace.com published on january 30, 20...</td>\n",
       "      <td>0.025605</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2019-02-02 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>4251</td>\n",
       "      <td>but before shareholders complain about the com...</td>\n",
       "      <td>0.029098</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>2019-02-02 01:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4252 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  \\\n",
       "0              0  nape summit week will also feature the annual ...   \n",
       "1              1  from apples hugely anticipated iphone x to sam...   \n",
       "2              2  they are not just the largest browser, but the...   \n",
       "3              3  the humanitarian crisis in the drc has placed ...   \n",
       "4              4  cramer prefers finisar (nasdaq: fnsr ) after a...   \n",
       "...          ...                                                ...   \n",
       "4247        4247  this report just confirmed apple's china probl...   \n",
       "4248        4248  apple's (nasdaq: aapl ) spectacular rise and f...   \n",
       "4249        4249  so, lets see what to expect from the entertain...   \n",
       "4250        4250   investorplace.com published on january 30, 20...   \n",
       "4251        4251  but before shareholders complain about the com...   \n",
       "\n",
       "      sentiment  label           stock_time  \n",
       "0      0.050660   -1.0  2017-12-07 20:00:00  \n",
       "1      0.107128    1.0  2017-12-08 22:00:00  \n",
       "2      0.034394    1.0  2017-12-12 02:00:00  \n",
       "3      0.000000   -1.0  2017-12-12 22:30:00  \n",
       "4      0.035844   -1.0  2017-12-14 18:30:00  \n",
       "...         ...    ...                  ...  \n",
       "4247   0.013567    1.0  2019-02-01 22:30:00  \n",
       "4248   0.017423   -1.0  2019-02-01 23:00:00  \n",
       "4249   0.029321   -1.0  2019-02-01 23:30:00  \n",
       "4250   0.025605   -1.0  2019-02-02 00:00:00  \n",
       "4251   0.029098   -1.0  2019-02-02 01:00:00  \n",
       "\n",
       "[4252 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gh_VQ3OvTEy7"
   },
   "outputs": [],
   "source": [
    "# create dataframe and extract text and sentiment\n",
    "df_text = dataFrame['text']\n",
    "df_sentiment = dataFrame['sentiment']\n",
    "df_label_score = dataFrame['label']\n",
    "\n",
    "# Using CountVectorizer to generate ngrams\n",
    "# generate ngrams for 2,2\n",
    "# cv_ngram = CountVectorizer(max_df=0.8, min_df=0.02, stop_words=stop_words, ngram_range=(2,2))\n",
    "# X_count = cv_ngram.fit_transform(df_text)\n",
    "stop_words_list = list(stop_words)  # Convert the set to a list\n",
    "# Directly fill NaN values for the Series\n",
    "df_text = df_text.fillna('')\n",
    "\n",
    "# Now, use stop_words_list instead of stop_words\n",
    "cv_ngram = CountVectorizer(max_df=0.8, min_df=0.02, stop_words=stop_words_list, ngram_range=(2,2))\n",
    "X_count = cv_ngram.fit_transform(df_text)\n",
    "\n",
    "# TfIdf transform to determine freqency\n",
    "# add smoothing to avoid zero\n",
    "tfTransform = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "X_count = tfTransform.fit_transform(X_count).toarray()\n",
    "\n",
    "# pca dataset for ngrams and sentiment\n",
    "\n",
    "pca_data = PCA(n_components=3)\n",
    "pcaComponent = pca_data.fit_transform(X_count)\n",
    "\n",
    "# combining pca and sentiment\n",
    "lastXTraining = np.hstack((pcaComponent, np.atleast_2d(df_sentiment).T))\n",
    "\n",
    "# train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(lastXTraining, df_label_score, test_size=0.3, random_state=23)\n",
    "\n",
    "# minMax scale the data\n",
    "minMaxPreprocess = preprocessing.MinMaxScaler()\n",
    "min_max_x_train = minMaxPreprocess.fit_transform(X_train)\n",
    "min_max_x_test = minMaxPreprocess.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "hK3XbjZuUo7W",
    "outputId": "ca60d6fc-912f-4cbc-fea4-dabd0c72cc54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data is: 0.5164650537634409\n",
      "Accuracy on test data is: 0.5329153605015674\n",
      "Precision: 0.5383928571428571\n",
      "Recall: 0.8841642228739003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluation\n",
    "\n",
    "# logistic regression model\n",
    "logistic_regression_model = LogisticRegression(random_state=10)\n",
    "logistic_regression_model.fit(min_max_x_train,y_train)\n",
    "\n",
    "# train and test prediction\n",
    "Y_train_prediction = logistic_regression_model.predict(min_max_x_train)\n",
    "Y_test_prediction = logistic_regression_model.predict(min_max_x_test)\n",
    "\n",
    "# print train and test accuracy\n",
    "trainingAccuracy_Score = accuracy_score(y_train, Y_train_prediction)\n",
    "print('Accuracy on training data is: {0}'.format(trainingAccuracy_Score))\n",
    "testAccuracy_Score = accuracy_score(y_test, Y_test_prediction)\n",
    "print('Accuracy on test data is: {0}'.format(testAccuracy_Score))\n",
    "\n",
    "\n",
    "\n",
    "# display precision\n",
    "prec_score = precision_score(y_test, Y_test_prediction)\n",
    "print('Precision: {0}'.format(prec_score))\n",
    "\n",
    "# display recall\n",
    "rec_score = recall_score(y_test, Y_test_prediction)\n",
    "print('Recall: {0}'.format(rec_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qc1H-04hXzGf"
   },
   "outputs": [],
   "source": [
    "\n",
    "# train with ngrams\n",
    "df_text = dataFrame['text']\n",
    "df_label_score = dataFrame['label']\n",
    "\n",
    "# Using CountVectorizer to generate ngrams\n",
    "# generate ngrams for 2,2\n",
    "# cv_ngram = CountVectorizer(max_df=0.8, min_df=0.02, stop_words=stop_words, ngram_range=(2,2))\n",
    "# X_count = cv_ngram.fit_transform(df_text)\n",
    "stop_words_list = list(stop_words)  # Convert the set to a list\n",
    "# Directly fill NaN values for the Series\n",
    "df_text = df_text.fillna('')\n",
    "\n",
    "# Now, use stop_words_list instead of stop_words\n",
    "cv_ngram = CountVectorizer(max_df=0.8, min_df=0.02, stop_words=stop_words_list, ngram_range=(2,2))\n",
    "X_count = cv_ngram.fit_transform(df_text)\n",
    "\n",
    "# TfIdf transform to determine freqency\n",
    "# add smoothing to avoid zero\n",
    "tfTransform = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "X_count = tfTransform.fit_transform(X_count).toarray()\n",
    "\n",
    "# pca dataset for ngrams and sentiment\n",
    "pca_data = PCA(n_components=3)\n",
    "lastXTraining = pca_data.fit_transform(X_count)\n",
    "\n",
    "# train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(lastXTraining, df_label_score, test_size=0.3, random_state=23)\n",
    "\n",
    "# minMax scale the data\n",
    "minMaxPreprocess = preprocessing.MinMaxScaler()\n",
    "min_max_x_train = minMaxPreprocess.fit_transform(X_train)\n",
    "min_max_x_test = minMaxPreprocess.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "Hl7HU1K7YAQf",
    "outputId": "2a3519df-e443-4ff5-8f00-3523410b67a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data is: 0.5127688172043011\n",
      "Accuracy on test data is: 0.5470219435736677\n",
      "Precision: 0.5866666666666667\n",
      "Recall: 0.5161290322580645\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluation\n",
    "\n",
    "# logistic regression model\n",
    "logistic_regression_model = LogisticRegression(random_state=10)\n",
    "logistic_regression_model.fit(min_max_x_train,y_train)\n",
    "\n",
    "# train and test prediction\n",
    "Y_train_prediction = logistic_regression_model.predict(min_max_x_train)\n",
    "Y_test_prediction = logistic_regression_model.predict(min_max_x_test)\n",
    "\n",
    "# print train and test accuracy\n",
    "trainingAccuracy_Score = accuracy_score(y_train, Y_train_prediction)\n",
    "print('Accuracy on training data is: {0}'.format(trainingAccuracy_Score))\n",
    "testAccuracy_Score = accuracy_score(y_test, Y_test_prediction)\n",
    "print('Accuracy on test data is: {0}'.format(testAccuracy_Score))\n",
    "\n",
    "# display precision\n",
    "prec_score = precision_score(y_test, Y_test_prediction)\n",
    "print('Precision: {0}'.format(prec_score))\n",
    "\n",
    "# display recall\n",
    "rec_score = recall_score(y_test, Y_test_prediction)\n",
    "print('Recall: {0}'.format(rec_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qSWA4TPJ86OE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Train_LR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
